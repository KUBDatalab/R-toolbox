---
title: "Fintuning af store tekstmodeller"
teaching: 42
exercises: 42
---

::: questions
-   Men måske nok ikke så store igen.
:::

::: objectives
-   tune din model
:::




eksemplet herunder, der kommer fra chatbotten, skal som minimum oversættes til R

Der skal nok være en henvisning til noget med ucloud.

### Hvad er det?



# Installer reticulate hvis det ikke allerede er installeret
install.packages("reticulate")
library(reticulate)

# Opret et Python-virtualmiljø og installer nødvendige pakker
virtualenv_create("r-reticulate")
virtualenv_install("r-reticulate", packages = c("transformers", "datasets", "torch"))

# Brug det oprettede miljø
use_virtualenv("r-reticulate", required = TRUE)

# Kør Python-kode direkte fra R
py_run_string("
import transformers
import datasets
import torch

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

# Vælg en mindre model
model_name = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Indlæs dataset (IMDB bruges her som eksempel)
dataset = datasets.load_dataset('imdb')

# Funktion til tokenisering
def tokenize_function(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)

# Anvend tokenisering på hele datasættet
tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')
tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

# For at holde træningen hurtig bruges et mindre subset af data
train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(2000))
eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select(range(500))

# Opsæt træningsparametre
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=50,
    load_best_model_at_end=True
)

# Initialiser Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer
)

# Start træning
trainer.train()
")




::: keypoints
-   Man kan tune en model

:::