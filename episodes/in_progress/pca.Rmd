---
title: 'Principal Component Analysis'
teaching: 10
exercises: 2
---

:::::::::::::::::::::::::::::::::::::: questions 

- What is PCA?
- What is dimension reduction

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- FIXME

::::::::::::::::::::::::::::::::::::::::::::::::

## Dimension reduction

We call the concept of taking a dataset with many variables ("dimensions"), and
reducing it to fewer variables while keeping important information: "Dimension reduction".

Imagine a table with 100 columns (variables or features). A lot of them might
be correlated, noisy or redundant. Instead of working with all 100 features, we
might be able to identify or summarise the imparton structure or information
of the data, using only 5 or 6 features (or variables)

We could pick out the most important variables and use only those to describe the
data. This is more or less what we do when we make a multiple linear regression:
We try to explain the variability in the data, using a selection of a few variables.
But we might need a lot of variables to explain variability enough. Any two variables
might not be able to explain more than 10% of the variability.

An alternative approach could be to construct new variables by combining the 
original ones. Done right, it might be possible to construct 5 or 6 new
features that describe a lot of the variance.

This is what Principal Component Analysis does. It constructs, algorithmically
(or automatically) a set of new variables. It is not able to explain more
of the variance than the original 100 variables. But it migh be able to 
construct one new variable that explains 60% of the variability, and another,
second one, that explain 25%. Those two new features alone would then
explain a total of 85% of the variability of the data. 


```{r, include = FALSE}
library(tidyverse)
library(GGally)
library(ggExtra)
library(patchwork)
```

# Principal Component Analysis

There exist quite a few different methods for doing dimension reduction. One
of the more popular is Principal Component Analysis - or PCA.

PCA is a method for reducing dimensions of a dataset. 


Let us look at some data, in this case wine. 

VI SKAL NOK HAVE ET DOWNLOAD EKSEMPEL

```{r}
wine <- read_csv("../data/wine.data", col_names = FALSE)
```

```{r}
wine %>% group_by(X1) %>% 
  summarise(across(X2:X14, mean))
```




In the description we can read about what the individual values are. But the 
thesis is, that really matters is note the concentration of one chemical component.
But rather the "cultivar", the type of grape used for making the wine.


Vi laver density plots på dem alle. Så. Hvor hyppige er en given 
observation i hver af de tre cultivarer? Der er da variable, hvor
vi kan se at lave værdier hyppigt optræder for en bestemt cultivar,
og høje for en anden. Men overlappet er ret stort for alle 
variable.
```{r}
wine %>% 
  pivot_longer(cols = X2:X14) %>% 
  mutate(X1 = factor(X1)) %>% 
  ggplot(aes(x=value, color = X1, group = X1)) +
  geom_density() +
  facet_wrap(~name, scales = "free")
```


ideen er at vi prøver at konstruere nye variable som kombinationer 
af de oprindelige. Og ser om vi kan skille cultivarerne baseret på 
dem i stedet.



Lad os prøve at gøre det. Og lad os pille cultivarerne ud først

```{r}
cultivars <- wine %>% select(X1)
```


```{r}
wine_to_pca <- wine %>% select(-X1)
```

Vi har nu data uden facit. Og vi har facitlisten. Vi antager at den 
variation der er i data kan forklare cultivarerne.

PCA'en køres med funktionen promp.



```{r run-pca, echo  =F}
pca_wine <- prcomp(wine_to_pca, scale. = TRUE)
```

Og så kan vi plotte det. Vi får en del med i modellen, men lad os starte
med at se på de principiale komponenter. De ligger i pca_penguins$x.
Vi tilføjer oplysninger om pingvin-arten, og farvelægger efter netop den:
```{r pca-plot}
pca_wine$x %>% 
  as_tibble() %>% 
  mutate(cultivar = cultivars$X1) %>% 
  ggplot(aes(PC1, PC2, colour = factor(cultivar))) + 
  geom_point()

```

vupti - ret klar adskillelse af de tre cultivarer.

Hvad har modellen gjort? 

algoritmen beregner et nyt koordinatsystem for vinene. Og nye koordinater for
vinene i dette nye koordinatsystem.

Lad os kigge på den: 

```{r}
pca_wine %>% names()
```

Scale og center. 

```{r}
pca_wine$scale
```

```{r}
pca_wine$center
```


dette beskriver hvordan samtlige observationer i datasættet er blevet centreret
og skaleret. Vi bringer basalt set alle vores variable på en form hvor deres
middelværdi er 0, og deres standardafvigelse er 1. Center matcher middelværdien
for hver variabel. Når vi trækker den fra samtlige observationer, bliver middel
værdien af de nye værdier 0. Tilsvarende med scale. Her dividerer vi med
værdien i scale. 




```{r}
pca_wine$sdev
```

Så er der rotation. Det er beregningen af det nye koordinatsystem:

```{r}
pca_wine$rotation
```
PC1, er den første af 13 akser i det nye koordinatsystem. Den er en lineær kombination
af de oprindelige 13 akser. 

```{r}
pca_wine$center
```

```{r}
summary(wine)
```


```{r}
pca_wine$scale
```
Og så er der sdev
```{r}
pca_wine$sdev
```
Det er standardafvigelserne for de enkelte principale komponenter.  Det kan vi
tolke som den del af variationen i datasættet, der forklares af hver af komponenterne.



Vi skal gerne kunne tage en ny måling, og placere den korrekt baseret på
data. 

Jeg kan forstå at den skalerer - for vi har jo sat scale. = TRUE. hvorfor?

Vi har en helt konkret udfordring. En af variablene har en middelværdi på 746.
En anden har 0.3619. En relativ ændring på 5% på den ene svarer til 37. Og på den anden 0.018095. Ret små variationer
i den første, der måske er helt ubetydelige for at 
afgøre hvad der er hvad, overdøver ret store ændringer på
den anden. Derfor skalarer og centrerer vi.

Skal man altid det? Ja. Og nej.
Centrering skal man stort set altid. Derfor er 
center = TRUE default. Her behøver vi sjældet at tænke
over det. Det er nu en god ide at gøre det alligevel.
Skalering derimod - afhænger af situationen. Derfor 
er scale. = FALSE default. Her skal vi huske at tænke os 
om.

Centrering - Hvis ikke vi centrerer vores data omkring 0,
vil den første pc pege mod gennemsnittet. Og det giver sjældent mening.
Skalering er vigtig når variablene har forskellig størrelsesorden. Eller er i forskellige enheder. Her har vi
begge dele.
Hvis alle variable er i samme enhed og størrelsesorden,
behøver vi ikke skalere. På den anden side skader det ikke
at gøre det, og vi sparer ikke nogen nævneværdig tid ved
at gøre det selvom det er overflødigt.

Men hvis vi ønsker at resultatet skal afspejle den absolutte
varians, ikke den relative, kan vi vælge ikke at skalere.
Og i en del fag, økonomi, teknik og geologi, er den 
høje varians faktisk vigtig. Og derfor kan vi vælge ikke at 
skalere.

Kan vi klassificere? Jep. Men det gør vi ved at køre en klassifikationsalgo på 
resultaterne af pca'en.

## Hvad ligger der ellers i modellen?

PCA-modellen for pingvinerne hedder pca_penguins. 

Den indeholder flere ting. I x ligger de nye værdier for alle 
pingvinerne i de nye dimensioner, kaldet Principiale Komponenter:
```{r}
pca_penguins$x %>% head()
```


Hvad havde den første pingvin af data?
```{r}
data_penguins %>% slice(1)
```

Og hvor blev den placeret i de nye koordinater?
```{r}
pca_penguins$x[1,]
```
Hvordan fandt vi dem?
```{r}
pca_penguins$rotation
```

```{r}
(43.99279-39.1)* 0.4537532 - (17.16486-18.7)*0.3990472 + (200.96697-181)*0.5768250+ (4207.05706-3750)*0.5496747
```
```{r}

```


::::spoiler
## What does scaling and centering look like?

We can scale or not. And we can center. Or not. The four combinations in the
example with data on wine looks like this:

```{r echo = FALSE}
pca_ct_st <- prcomp(wine_to_pca, center = TRUE, scale. = TRUE)$x %>% 
  as_tibble() %>% 
  mutate(cultivar = cultivars$X1)
pca_ct_sf <- prcomp(wine_to_pca, center = TRUE, scale. = FALSE)$x %>% 
  as_tibble() %>% 
  mutate(cultivar = cultivars$X1)
pca_cf_st <- prcomp(wine_to_pca, center = FALSE, scale. = TRUE)$x %>% 
  as_tibble() %>% 
  mutate(cultivar = cultivars$X1)
pca_cf_sf <- prcomp(wine_to_pca, center = FALSE, scale. = FALSE)$x %>% 
  as_tibble() %>% 
  mutate(cultivar = cultivars$X1)

ct_st <- pca_ct_st %>% 
  ggplot(aes(PC1, PC2, colour = factor(cultivar))) + 
  geom_point() +
  ggtitle("Both centering and scaling") +
  theme(legend.position = "none")

ct_sf <- pca_ct_sf %>% 
  ggplot(aes(PC1, PC2, colour = factor(cultivar))) + 
  geom_point() +
  ggtitle("Centering, no scaling") +
  theme(legend.position = "none")

cf_st <- pca_cf_st %>% 
  ggplot(aes(PC1, PC2, colour = factor(cultivar))) + 
  geom_point() +
  ggtitle("Scaling, no centering") +
  theme(legend.position = "none")

cf_sf <- pca_cf_sf %>% 
  ggplot(aes(PC1, PC2, colour = factor(cultivar))) + 
  geom_point() +
  ggtitle("Neither scaling, nor centering") +
  theme(legend.position = "none")

ct_st + ct_sf + cf_st + cf_sf

```

In this case, it is clear that we should both scale and center our data
to get the best separation between the three cultivars
::::


%>% 
  ggplot(aes(PC1, PC2, colour = factor(cultivar))) + 
  geom_point()


::::::::::::::::::::::::::::::::::::: keypoints 

- Use `.md` files for episodes when you want static content
- Use `.Rmd` files for episodes when you need to generate output
- Run `sandpaper::check_lesson()` to identify any issues with your lesson
- Run `sandpaper::build_lesson()` to preview your lesson locally

::::::::::::::::::::::::::::::::::::::::::::::::

