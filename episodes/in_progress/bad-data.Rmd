---
title: 'Bad data'
teaching: 10
exercises: 2
---

:::::::::::::::::::::::::::::::::::::: questions 

- How do you write a lesson using R Markdown and `{sandpaper}`?

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Explain how to use markdown with the new lesson template
- Demonstrate how to include pieces of code, figures, and nested challenge blocks

::::::::::::::::::::::::::::::::::::::::::::::::

A collection of problems with data

Real world data is usually messy. Some of these problems have to be fixed by
the source of the data. Some can be fixed by you. Others you might need help
for.

This is a collection of some, perhaps most, of the problems we might encounter
in the real world.

Vi skal nok have det delt op i to eller tre

## Problems that should be solved at the source

### Missing values

If a value is missing, ask youself: "Do I know why that value is missing?".
Did a respondent refuse to answer a 
question? Is it because it was never collected? If it was never collected, why?

### Zeroes replace missing values 

If a value is missing, at least we know that it _is_ missing. A worse situation
is if an arbitrary value is used to indicate, or even replace, a missing value.
It can happen because a human did not think about the implications, or because
automated datacollection did not know how to handle missing values. If you see
zeroes in your data, ask yourself if the value is truly 0. Or if someone or 
something added 0 to indicate a missing value. 

Other weird numbers might be used as well. The author of this text tended to 
use "999" to indicate that a measurement, that could only take values between
0 and 1, was missing. 

Remember that 0 might not present as "0". A false 0-value in a date-time format
will often appear as 1970-01-01T00:00:00Z or 1969-12-31T23:59:59Z because dates 
and times are regularly represented as a number of seconds since a specific
timestamp. For geographical data it might show up as 0degre00'00.0"N+0degree00'oo.o"E
or 0degreeN0degreeE. Automated systems might recognize a missing value, but 
impute it based on knowledge of the country by assigning it to a predetermined 
value for that country.
[This has caused real-life problems for people](https://www.youtube.com/watch?v=vh6zanS_epw).



### Data is missing - but should be there

If you know what the data is about - you should do sanity checks on the data. If 
the data is about Danish municipalities, there should be 98 municipalities in
the data. That is, if it is current; reforms were made in 2003 and 2007, affecting 
the true number. If you have data on the US, there should be 50 states. But check
if US territories occur in the data. If Puerto Rico is included, 50 is not the 
correct number.



### Duplicated rows or values

If the same row appears more than once in your data, you should figure out why.
Sometimes a value in a column might be duplicated. Often this is perfectly fine;
if we have more than two rows in a dataset collecting the sex of respondents, we
should expect duplicated values. And in a tidy dataset we expect an ID-column to
contain repeated values. But if something that you expect to be unique, isn't,
you should find out why.

### Inconsistent spelling

Working with text, the phenomenon of spelling mistakes shows up. "København",
"Koebenhavn", "Kobenhavn" and "Copenhagen" represent the same city.
This most  frequently occurs when the data has been entered manually by humans. 
And if data has been entered by humans, the probability of other errors is high.


### Inconsistent name order

If your data contains Middle Eastern or East Asian names, there might be 
problems with the identification of surnames. In many cultures the family name
precedes the given name. In a western setting some people from these cultures
might chose to reverse the order of their name, in order to adhere to the western
standard of given names first, family name last. Others might not. Be careful
not to assume that you can create `first_name` and `last_name` columns based
simply on position. To complicate the issue further, the use of mononyms, 
having a name composed of only one word, exists in certain countries.

### Inconsistent date formats

There exists a correct way of writing dates. But very few people actually use
it. An example is "11/10/15". In the US this would denote the date November
10th, 2015, but in most of the rest of the world it would denote October 11th
2015. To further complicate matters, other calendars than the gregorian exists, 
and in an islamic setting you might have to know exactly _where_ the date was 
registered in order to take into account local astronomical observations of the moon. 


### Unspecified units

If your data comes from a serious source, you might expect the column "weight"
to contain metric units. But is it kg or g? And are you absolutely sure that
it is actually in metric, and not imperial? If the values are metric you will
still have to consider the unit. The metric SI-unit for concentration should
be mol/m3. But the convention might be ng/ml. Be very aware of values indicating
monetary value. Is it in USD or EUR? And remember the concept of inflation. An
US dollar in 2025 did not have the same value as it did in 2024. Always figure
out what the units of your data is.

### Bad choices for categories

Be careful with data that is registered as only TRUE or FALSE, but really is not.
For survey data, "refused" or "no answer" might be valid responses. The "other"
category is also a source of problems. Does other mean that the person collecting
data did not know the answer, or is it simply not covered by other possibilities
for answers? And be extremely careful if the definitions tend to be arbitrary,
like race or ethnicity.

### Ambiguous field names

That variable name `date`. What date does that actually refer to? Is it date of
birth or the date the data was recorded? 
A variable name ´residence´ might refer to the place a person lives. Or the 
place a person pay taxes. Make sure that you know exactly what a field name
that can cover more than one meaning actually mean.

### Uncertain provenance


### Presence of suspicious values

### To coarse data

### Aggregates or totals are inconsistent

### 65536 or 1048576 rows in the spreadsheet

Data that has passed through an Excel spreadsheet can be corrupted in several
ways. Older versions of Excel allowed a maximum of 65536 rows. If your dataset
have that number of rows (including headers), chances are that it has passed
through Excel, and been truncated. In that case you are probably missing data.
Newer versions of Excel have a maximum of 1048576 rows. The chances of 
getting your data truncated by Excel are lower, but if that is the number of rows
in your data, you should be suspicious.


### Spreadsheet dates in 1900, 1904, 1969 or 1970

### Values that could be interpreted as dates

If you have data that Excel can interpret as data, be careful. If that data
has passed through Excel, it might have been corrupted. This has been a major
problem with names for genes. The "Membrane Associated Ring-CH-Type Finger 1" gene
(which plays a role in the human immune system) used to be called, for short,
MARCH1. Excel looked at this, recognised it as march 1st, and stored it as 
a date. The problem have, sort off, been solved by renaming the gene MARCHF1,
fooling Excel. And Microsoft in 2023, added a setting in Excel turning off automatic
data conversion. A [study in 2016](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1044-7) found that approximately 20% of studied published
papers had been affected by Excel errors.

### Text has been converted to numbers

## Problems that you should solve

### Garbled text

### Garbled line endings

### Data are in a PDF

### Too granular data

### Data entered by humans

### Data is intermingled with formatting and annotations

### Aggregations computed on missing values

### Non-random sample

### Too large margin of error

### Unknown margin of error

### Biased sample

### Manually edited data

### Data skewed by inflation

### Data skewd by season or natural variation

### Manipulated timeframe

### Manipulated frame of reference

## Someone else should probably help you

### Untrustworthy author

### Opaque collection process

### Data asserts unrealistic precision

### Inexplicable outliers

### Index masking underlying variation

### Suspected p-hacking

### Benford's law fails

### It's too good to be true

### data are aggregated to the wrong categories or geographies

### data are in scanned documents



::::::::::::::::::::::::::::::::::::: keypoints 

- Use `.md` files for episodes when you want static content
- Use `.Rmd` files for episodes when you need to generate output
- Run `sandpaper::check_lesson()` to identify any issues with your lesson
- Run `sandpaper::build_lesson()` to preview your lesson locally

::::::::::::::::::::::::::::::::::::::::::::::::

