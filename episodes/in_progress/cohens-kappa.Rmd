---
title: 'Cohens Kappa'
teaching: 10
exercises: 2
---

:::::::::::::::::::::::::::::::::::::: questions 

- How do you write a lesson using R Markdown and `{sandpaper}`?

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Explain how to use markdown with the new lesson template
- Demonstrate how to include pieces of code, figures, and nested challenge blocks

::::::::::::::::::::::::::::::::::::::::::::::::

## What do we use it for?

We use Cohens Kappa (or $\kappa$), to compare if two sets of categorisations
is done reliably.

Imagine a set of images. We ask two persons to answer a simple question - is there
a cat on the images. To what degree do the two persons agree on the presence 
or absence of cats?

That seems simple enough. However if they agree that there is a cat, this might
indicate that they are both good at identifying cats, and of course agrees. 
But they might just as well be very bad at recognizing cats - but agree by
random chance. 


## Where is it used?

*Inter-Rater Reliability*. That is the example from earlier. To what degree do two "raters" agree
on their rating or classification of something? This is useful in subjects and
research where subjective assessments are important, such as psychology, medicine
and languages.

*Intra-Rater Reliability*. Here we test how consistent a single rater is. Are my
ratings from a week ago, consistent with the ratings I make today?

*Comparison of diagnostic tests*. It can be used to compare the results of two
different diagnostic tests for some medical condition. Does the cheaper, less
invasive test, give the same results as the more expensive invasive test?

*Natural language processing* (NLP). When we categorise textual data, eg, evaluating
the sentiment of a text, or identify named entities - how well does two methods 
or persons agree?

*Educational research*. Will two teachers give the same grade to the same papers
turned in by students.

In humanities we are concerned with intersubjectively transferability - based
on available data, are the subjective evaluation of two different persons 
in agreement.

## Limitations

Cohens $\kappa$ can be used directly for two nominal (no order of the classes) 
categorical variables. 
The main limitation is that Cohens $\kappa$ only applies for exactly two
raters.

## How do we calculate it?

The library `vcd` contains a function `Kappa()` that does the heavy lifting.

It needs a so called confusion matrix. Let us begin by constructing one.

::::spoiler
## Constructing the data

Ja, dette bør ligge i et datasæt der kan indlæses.
```{r}
library(tidyverse)
diagnoses <- tribble(~rater1, ~rater2,
  "dep", "dep",
  "dep", "dep",
  "dep", "dep",
  "dep", "dep",
  "dep", "dep",
  "dep", "dep",
  "dep", "dep",
  "dep", "dis",
  "dep", "sch",
  "dep", "sch",
  "dep", "neu",
  "dep", "neu",
  "dep", "neu",
  "dis", "dis",
  "dis", "dis",
  "dis", "dis",
  "dis", "dis",
  "dis", "dis",
  "dis", "dis",
  "dis", "dis",
  "dis", "dis",
  "dis", "sch",
  "dis", "sch",
  "sch", "sch",
  "sch", "sch",
  "neu", "neu",
  "oth", "oth",
  "oth", "oth",
  "oth", "oth",
  "oth", "oth"
  )
```

::::

The object `diagnoses` now contains 30 observations. Imagine two psychiatrists 
both evaluating 30 patients. They diagnose one of these disorders: Depression
(dep), Schizophrenia (sch), Dissociative disorder (dis), Neurosis (neu) and
Other (oth). To what degree do they agree?

```{r vis_data}
head(diagnoses)
```
```{r}
conf_table <- table(diagnoses)
conf_table
```
This is the confusion matrix. 7 patients are diagnosed with depression by
both raters (psychiatrists). But looking at Schizophrenia, we see that two
patients are diagnosed with Schizophrenia by rater2, but with Depression by
rater1.

This is the table we need to provide to the `Kappa()` function:

```{r}
library(vcd)
res.k <- Kappa(conf_table)
res.k
```

::::spoiler
## Unweighted vs Weighted?

For some use cases it does not really matter what kind of disagreement 
the two raters have. They of course should agree, but the disagreement of 
classifying a Golden Retriver as a Labrador Retriver, is not as serious as 
mistaking it for a Chihuahua. 

The Unweighted $\Kappa$ is calculated with the
assumption that the different disagreements are of equal seriousness. 

The Weighted $\Kappa$ assumes that some disagreements are worse than others.

::::

## How confident are we in $\Kappa$?

The function `confint()` will give us the 95% confidence interval of the $\Kappa$
calculated:

```{r confidence_interval}
confint(res.k)
```
## Interpretation

Cohens $\Kappa$ takes into account the possibility that the two raters agree
by chance. This makes it a more robust measure of agreement than simply looking
at the precent agreement. It returns a value between -1 and 1, 1 indicating
perfect agreement, 0 no agreement besides random chance, and negative values
less agreement than we would expect by chance.

So. Is an unweighted $\Kappa$ of 0.6507 good or bad?

That depends on what we are actually classifying. Classifying psychiatric disorders
is obviously more important than classifying dog breeds. 


[Fleiss et al (2003)](https://onlinelibrary.wiley.com/doi/book/10.1002/0471445428), 
argues that the rules-of-thumb in this table are good indications:

```{r valuevurdering, echo = F}
tribble(~`Value of K`, ~`Level of agreement`,
        ">0.75", "Excellent agreement beyond chance",
        "0.4> value < 0.75", "Fair to good agreement beyond chance",
        "<0.4", "Poor agreement beyond chance"
        ) %>% 
  knitr::kable()
```

Whereas [McHugh (2012)](https://doi.org/10.11613/BM.2012.031) presents this
table:

```{r McHugh_vurdering, echo =F}
tribble(~`Value of K`, ~`Level of agreement`, ~`% of data that are reliable`,
        "0 - 0.20", "None", "0 - 4%",
        "0.21 - 0.39", "Minimal", "4 - 15%",
        "0.40 - 0.59", "Weak", "15 - 35%", 
        "0.60 - 0.79", "Moderate", "35 - 63%",
        "0.80 - 0.90", "Strong", "64 - 81%",
        "Above 0.90", "Almost Perfect", "82 - 100%"
        ) %>% 
  knitr::kable(align = "llr")
```

The estimates of the % of data that are reliable depends on the distribution of
the classifications, and the values in this table should be taken with a grain of
salt.




## Assumptions

Cohens $\Kappa$ have the following requirements:

* Two raters (or coders). No more, no less.

* Nominal scale – there are no order to the classifications. 

* Same set of items – both raters rate the exact same cases.

* Independence of ratings – the two ratings must not influence each other.

* Mutually exclusive categories – an arbitrary case can only get _one_ classification.








## The math

We define $\Kappa$ as:

Null hypotesen, kappa = 0, hvis der er enighed, er det tilfældigt.
Den alternative hypotese, kappa != 0, Enigheden er forskellig fra tilfældigheder.

Kappa er defineret som:

$$\Kappa = P_0 - \frac{P_e}{1-P_e}$$


$P_0$ er andelen af observeret enighed, $P_e$ er andelen af tilfældig enighed.

Når man ser hvordan den beregnes, lugter det af $\chi^2$-testen. 
Det er ikke et tilfælde:
Feingold, Marcia (1992). "The Equivalence of Cohen's Kappa and Pearson's Chi-Square Statistics in the 2 × 2 Table." Educational and Psychological Measurement 52(1): 57-61. <http://hdl.handle.net/2027.42/67443>

Hvis der er to raters og to udfald, er test statistikken for Kappa den samme
som for Pearsons chi i anden. Så de er i familie.

Anyway, definitionen betyder at værdierne kan gå fra -1 til 1. Hvis den er 0,
er enigheden ikke bedre end hvad vi ville få tilfældigt. Ved negative værdier
er den mindre end hvad vi ville få tilfældigt. Hvis positiv er enigheden større
end ved tilfældighed.


```{r}
##        Doctor2
## Doctor1 Yes No Total
##   Yes   a   b  R1   
##   No    c   d  R2   
##   Total C1  C2 N
```

a, b, c og d er de observerede, talte, værdier
$R_1 = a + b$ 
$R_2 = c + d$
$C_1 = a + c$
$C_2 = b + d$
og

ad $N = a + b + c + d$ adf 



$$P_0 = \frac{a + c}{N}$$

Det var den lette.

$P_e$ finder vi ved at finde sandsynligheden for at begge raters tilfældigt siger ja.

Doctor1 siger Ja til $\frac{R_1}{N}$ tilfælde.
Doctor2 siger Ja til $\frac{C_1}{N}$ tilfælde
Sandsynligheden for at de tilfældigt begge siger ja er 
$R1/N * C1/N$


Så finder vi sandsynligheden for at de begge tilfældigt siger nej
Doctor1 siger nej til $R_2/N$
Doctor2 siger nej til $\frac{C_2}{N}$
Sandysnligheden for at de begge tilfældigt siger nej er:
$R_2/N * C_2/N$

Sandsynligheden for at de er enige er derfor:
$R_1/N * C_1/N + R_2/N * C_2/N$

Så har vi Pe. Og kan beregne Kappa som:
$\kappa = ((a+d)/N - (R_1/N * C_1/N + R_2/N * C_2/N))   / 1 - (R_1/N * C_1/N + R_2/N * C_2/N)$

Det bliver noget mere langt, men ikke nødvendigvis langhåret, hvis der er 
mere end en kategori.

Der er en formel for SE. 

$SE_\kappa = \frac{P_e + P_2^2 - \sum_i P_{i+}P_{+i}(P_{i+} + P_{+i})}{N(1+ P_e)^2}$

Og så kan der beregnes konfidensintervaller på sædvanligvis efter normalfordelingen:

$\kappa +- Z_{\alpha/2} SE_\kappa$

i+ og +i er række og kolonne summerne.



## Alternatives

Cohens $\Kappa$ only works (directly) for nominal values, and two raters.
The option of weighting the data give us some options for using it on ordinal
values, but more direct options exist.

* Weighted $\Kappa$ is used for ordinal values. We use the function `kappa2()` from the `irr` package.
* Light's $\Kappa$ is used for studying interrater agreement between more than 2 raters. We use the function `kappamlight()` from the `irr` package.
* Fleiss $\Kappa$ is also used when having more than 2 raters. *But* does _not_
require the same raters for each subject.



::::::::::::::::::::::::::::::::::::: keypoints 

- Use `.md` files for episodes when you want static content
- Use `.Rmd` files for episodes when you need to generate output
- Run `sandpaper::check_lesson()` to identify any issues with your lesson
- Run `sandpaper::build_lesson()` to preview your lesson locally

::::::::::::::::::::::::::::::::::::::::::::::::

